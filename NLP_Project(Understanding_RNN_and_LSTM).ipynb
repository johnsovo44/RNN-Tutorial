{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Project(Understanding RNN and LSTM)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnsovo44/RNN-Tutorial/blob/master/NLP_Project(Understanding_RNN_and_LSTM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M-YkEtE_AB_",
        "colab_type": "code",
        "outputId": "33b6f517-4088-464c-b3a3-f1869b8eea34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YGIcehm_GFS",
        "colab_type": "code",
        "outputId": "fad40fee-9632-475e-be5d-cdb2b18e7139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79_8FlmO_N0X",
        "colab_type": "text"
      },
      "source": [
        "# Purpose\n",
        "\n",
        "I'lll be following along with Jason Brownlee's article on how LTSM is implemented. To differentiate my work from his, he has laid out some extension ideas at the end help with experimentation. I will also use a different unstructured dataset once I have gotten an understanding of his work. \n",
        "\n",
        "Link to the article: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "By the end of this lesson I will know the following:\n",
        "\n",
        "- Where to download a free corpus of text to further train text generative models: [Project Gutenberg](https://www.gutenberg.org/) (experiments are just limited to text either. You can generate source code, LaTex, HTML or Markdown and more)\n",
        "- How to frame the problem of text sequences to a recurrent neural network generative model.\n",
        "- How to develop an LSTM to generate plausible text sequences for a given problem. \n",
        "\n",
        "For additional understanding of RNN models please watch this video: [Afr iendly Introduction to Recurrent Neural Networks](https://www.youtube.com/watch?v=UNmqTiOnRfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmieu4A1MAp",
        "colab_type": "text"
      },
      "source": [
        "# Approach\n",
        "\n",
        "\n",
        "\n",
        "1.   Write out code\n",
        "2. Troubleshoot\n",
        "2.   Comment out code and what is happening at each step\n",
        "3. Experiment with extension ideas\n",
        "4. Insert in own dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLflahBiyPpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4NZZJd20hc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "ac9b7d0e-d68e-4208-f73a-1e6839c093ce"
      },
      "source": [
        "filename = \"C:\\Users\\Vonn\\Downloads\\wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding = 'utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-20f0331f0fae>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    filename = \"C:\\Users\\Vonn\\Downloads\\wonderland.txt\"\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5-kArAf5fQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_text = \"\"\"Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\n",
        "\n",
        "This eBook is for the use of anyone anywhere at no cost and with\n",
        "almost no restrictions whatsoever.  You may copy it, give it away or\n",
        "re-use it under the terms of the Project Gutenberg License included\n",
        "with this eBook or online at www.gutenberg.org\n",
        "\n",
        "\n",
        "Title: Alice's Adventures in Wonderland\n",
        "\n",
        "Author: Lewis Carroll\n",
        "\n",
        "Posting Date: June 25, 2008 [EBook #11]\n",
        "Release Date: March, 1994\n",
        "[Last updated: December 20, 2011]\n",
        "\n",
        "Language: English\n",
        "\n",
        "\n",
        "*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ALICE'S ADVENTURES IN WONDERLAND\n",
        "\n",
        "Lewis Carroll\n",
        "\n",
        "THE MILLENNIUM FULCRUM EDITION 3.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CHAPTER I. Down the Rabbit-Hole\n",
        "\n",
        "Alice was beginning to get very tired of sitting by her sister on the\n",
        "bank, and of having nothing to do: once or twice she had peeped into the\n",
        "book her sister was reading, but it had no pictures or conversations in\n",
        "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
        "conversations?'\n",
        "\n",
        "So she was considering in her own mind (as well as she could, for the\n",
        "hot day made her feel very sleepy and stupid), whether the pleasure\n",
        "of making a daisy-chain would be worth the trouble of getting up and\n",
        "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
        "close by her.\n",
        "\n",
        "There was nothing so VERY remarkable in that; nor did Alice think it so\n",
        "VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
        "Oh dear! I shall be late!' (when she thought it over afterwards, it\n",
        "occurred to her that she ought to have wondered at this, but at the time\n",
        "it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\n",
        "OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\n",
        "Alice started to her feet, for it flashed across her mind that she had\n",
        "never before seen a rabbit with either a waistcoat-pocket, or a watch\n",
        "to take out of it, and burning with curiosity, she ran across the field\n",
        "after it, and fortunately was just in time to see it pop down a large\n",
        "rabbit-hole under the hedge.\n",
        "\n",
        "In another moment down went Alice after it, never once considering how\n",
        "in the world she was to get out again.\n",
        "\n",
        "The rabbit-hole went straight on like a tunnel for some way, and then\n",
        "dipped suddenly down, so suddenly that Alice had not a moment to think\n",
        "about stopping herself before she found herself falling down a very deep\n",
        "well.\n",
        "\n",
        "Either the well was very deep, or she fell very slowly, for she had\n",
        "plenty of time as she went down to look about her and to wonder what was\n",
        "going to happen next. First, she tried to look down and make out what\n",
        "she was coming to, but it was too dark to see anything; then she\n",
        "looked at the sides of the well, and noticed that they were filled with\n",
        "cupboards and book-shelves; here and there she saw maps and pictures\n",
        "hung upon pegs. She took down a jar from one of the shelves as\n",
        "she passed; it was labelled 'ORANGE MARMALADE', but to her great\n",
        "disappointment it was empty: she did not like to drop the jar for fear\n",
        "of killing somebody, so managed to put it into one of the cupboards as\n",
        "she fell past it.\"\"\"\n",
        "\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "# this is here because the file will not open right now. Need to troubleshoot\n",
        "# in second phase. But this is here so I can see what needs troubleshooting. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RrLPwoP1IG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c,i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnDWbJPr5_0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "outputId": "74ec66c8-c311-4174-f920-92a733f8e2af"
      },
      "source": [
        "char_to_int"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '#': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " '*': 7,\n",
              " ',': 8,\n",
              " '-': 9,\n",
              " '.': 10,\n",
              " '0': 11,\n",
              " '1': 12,\n",
              " '2': 13,\n",
              " '3': 14,\n",
              " '4': 15,\n",
              " '5': 16,\n",
              " '8': 17,\n",
              " '9': 18,\n",
              " ':': 19,\n",
              " ';': 20,\n",
              " '?': 21,\n",
              " '[': 22,\n",
              " ']': 23,\n",
              " 'a': 24,\n",
              " 'b': 25,\n",
              " 'c': 26,\n",
              " 'd': 27,\n",
              " 'e': 28,\n",
              " 'f': 29,\n",
              " 'g': 30,\n",
              " 'h': 31,\n",
              " 'i': 32,\n",
              " 'j': 33,\n",
              " 'k': 34,\n",
              " 'l': 35,\n",
              " 'm': 36,\n",
              " 'n': 37,\n",
              " 'o': 38,\n",
              " 'p': 39,\n",
              " 'q': 40,\n",
              " 'r': 41,\n",
              " 's': 42,\n",
              " 't': 43,\n",
              " 'u': 44,\n",
              " 'v': 45,\n",
              " 'w': 46,\n",
              " 'x': 47,\n",
              " 'y': 48}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7cufhWx4eJ9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "fe75ffcb-288a-4365-e780-8d7be4d07a1d"
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"total Vocab: \", n_vocab)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  3084\n",
            "total Vocab:  49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkHMTtPL50AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4HiVmV6RwT",
        "colab_type": "text"
      },
      "source": [
        "So N distinct vocab characters are able to produce N characters for the full text. There are obviously characters in here that could be removed in future cleaning but first let's see how they will play a role in the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uBhWjr86aBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2gvInwG6z0o",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Training Data\n",
        "\n",
        "(possible area to revisit given flexibility in choosing how to break up the text and exposing it to the network during training. In the tutorial wea re going to split the text in traunches of 100 characters. In the future you could try just the words or sentences.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc9iMeKO9KsY",
        "colab_type": "text"
      },
      "source": [
        "A Note on training: \"The way it works is each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output(y).\" A window is sliding arcoss 100 characters and then guessing the next output (this goes for every character after the first 100 characters). 100 characters is not standard.\n",
        "\n",
        "An example of this is the word chapter (from the tutorial):\n",
        "\n",
        "If we had a 5 character window instead...\n",
        "\n",
        "X = chapt\n",
        "\n",
        "y = e\n",
        "\n",
        "next one...\n",
        "\n",
        "X = hapte\n",
        "\n",
        "y = r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd2b0g_V-FI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7415f8c3-f679-4a09-fb9a-aa0e6773156e"
      },
      "source": [
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  2984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRJq1QbgAF1I",
        "colab_type": "text"
      },
      "source": [
        "The tutorial ended up having the 100 less patterns as they did characters. So if you do this again, make sure that holds true for the full text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FTuXGG8AZcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCvAxMTqAf9o",
        "colab_type": "text"
      },
      "source": [
        "# Prepare for Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K02bPV6CAjXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}